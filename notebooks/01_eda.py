# -*- coding: utf-8 -*-
"""01_eda (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r2kkKz-W3za_1xBrp9CGL7MDCaE5cfD6
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re

df = pd.read_csv('/content/vehicle_feedback.csv')

# 1. Load the dataset
df = pd.read_csv('/content/vehicle_feedback.csv')

print(df.head())

#2. Check Class Balance (Is the data fairly distributed across classes?)
print (df['feedback'].value_counts())

## Check Class Balance
# Show balance for sentiment (text)
print("Sentiment (text) balance:")
print(df['sentiment (text)'].value_counts())
print("\nSentiment (binary) balance:")
print(df['sentiment (binary)'].value_counts())
print("\nCategory (text) balance:")
print(df['category (text)'].value_counts())
print("\nCategory (binary) balance:")
print(df['category (binary)'].value_counts())

"""#### 3. Look for "Dirty" Text and Check Text Lengths
Let's inspect the feedback column for unusual characters and extremely short texts, which may indicate data quality issues.
"""

# Check for unusual characters in feedback
import string
unusual_pattern = r'[^\w\s.,!?@#$%&*()\-+=:;\'\"/\\]'
feedback_with_unusual = df[df['feedback'].str.contains(unusual_pattern, regex=True, na=False)]
print(f"Rows with unusual characters in feedback: {len(feedback_with_unusual)}")
print(feedback_with_unusual['feedback'].head())

# Check for extremely short feedback texts
short_feedback = df[df['feedback'].str.len() < 10]
print(f"Rows with extremely short feedback (<10 chars): {len(short_feedback)}")
print(short_feedback['feedback'].head())

"""#### 4. Clean Unusual Characters from Feedback"""

# Remove unusual characters from feedback
# Corrected pattern to remove characters that are NOT alphanumeric, whitespace, or common punctuation
clean_pattern = r"[^a-zA-Z0-9\s.,!?\"'\-]"
df['feedback_clean'] = df['feedback'].str.replace(clean_pattern, '', regex=True)

# Show a sample of cleaned feedbacks
print(df[['feedback', 'feedback_clean']].head())

# Diagnostic: Test predict_sentiment function with actual X_test samples
print("\n--- Testing predict_sentiment with X_test samples ---")

# Ensure X_test, y_test_ids, and id2label are available from previous runs
if 'X_test' not in globals() or 'y_test_ids' not in globals() or 'id2label' not in globals():
    print("Error: Required variables (X_test, y_test_ids, id2label) not found. Please ensure all previous cells were run.")
else:
    # Select a few diverse samples from the test set
    test_sample_indices = [0, 10, 20, 30, 40] # Pick diverse indices

    for i in test_sample_indices:
        if i < len(X_test):
            test_text = X_test.iloc[i]
            true_label_id = y_test_ids[i]
            true_label = id2label[true_label_id]

            predicted_label, probs = predict_sentiment(test_text)

            print(f"\nTest Sample (first 100 chars): {test_text[:100]}...")
            print(f"True Sentiment: {true_label}")
            print(f"Predicted Sentiment (ONNX): {predicted_label}")
            print(f"Probabilities: {probs}")
            print("-" * 50)
        else:
            print(f"Index {i} is out of bounds for X_test.")

# Diagnostic: Test predict_sentiment function with actual X_test samples
print("\n--- Testing predict_sentiment with X_test samples ---")

# Ensure X_test, y_test_ids, and id2label are available from previous runs
if 'X_test' not in globals() or 'y_test_ids' not in globals() or 'id2label' not in globals():
    print("Error: Required variables (X_test, y_test_ids, id2label) not found. Please ensure all previous cells were run.")
else:
    # Select a few diverse samples from the test set
    test_sample_indices = [0, 10, 20, 30, 40] # Pick diverse indices

    for i in test_sample_indices:
        if i < len(X_test):
            test_text = X_test.iloc[i]
            true_label_id = y_test_ids[i]
            true_label = id2label[true_label_id]

            predicted_label, probs = predict_sentiment(test_text)

            print(f"\nTest Sample (first 100 chars): {test_text[:100]}...")
            print(f"True Sentiment: {true_label}")
            print(f"Predicted Sentiment (ONNX): {predicted_label}")
            print(f"Probabilities: {probs}")
            print("---")
        else:
            print(f"Index {i} is out of bounds for X_test.")

"""#### 5. Check for Duplicates and Missing Values"""

# Check for duplicate feedback entries
num_duplicates = df.duplicated(subset=['feedback']).sum()
print(f"Number of duplicate feedback entries: {num_duplicates}")

# Remove duplicates if any
if num_duplicates > 0:
    df = df.drop_duplicates(subset=['feedback']).reset_index(drop=True)
    print("Duplicate feedback entries removed.")
else:
    print("No duplicate feedback entries found.")

# Check for missing values in important columns
important_cols = ['feedback', 'sentiment (text)', 'category (text)']
missing = df[important_cols].isnull().sum()
print("\nMissing values per important column:")
print(missing)

# Remove rows with missing values in important columns
if missing.sum() > 0:
    df = df.dropna(subset=important_cols).reset_index(drop=True)
    print("Rows with missing values in important columns removed.")
else:
    print("No missing values in important columns.")

"""#### 6. Summarize Data Types and Basic Statistics"""

# Print data types for all columns
print("Data types for all columns:")
print(df.dtypes)

# Summarize basic statistics for all columns
print("\nBasic statistics for all columns:")
print(df.describe(include='all'))

"""#### 7. Visualize the Distribution of Feedback Text Lengths"""

# Calculate feedback text lengths
if 'feedback_clean' in df.columns:
    df['feedback_length'] = df['feedback_clean'].str.len()
else:
    df['feedback_length'] = df['feedback'].str.len()

# Plot the distribution and save to reports/figures
plt.figure(figsize=(8, 5))
sns.histplot(df['feedback_length'], bins=30, kde=True)
plt.title('Distribution of Feedback Text Lengths')
plt.xlabel('Text Length (characters)')
plt.ylabel('Count')
plt.show()

# Show basic statistics for feedback_length
print(df['feedback_length'].describe())

# Identify outliers (e.g., texts longer than 99th percentile or shorter than 1st percentile)
q_low = df['feedback_length'].quantile(0.01)
q_high = df['feedback_length'].quantile(0.99)
outliers = df[(df['feedback_length'] < q_low) | (df['feedback_length'] > q_high)]
print(f"Number of outlier feedbacks (outside 1st-99th percentile): {len(outliers)}")
print(outliers[['feedback', 'feedback_length']].head())

# Show feedbacks with length greater than 750 characters
long_feedbacks = df[df['feedback_length'] > 750][['feedback', 'feedback_length']]
print(long_feedbacks)

# Show the feedback at index 7388
print(df.loc[7388, 'feedback'])

print(len(df.loc[7388, 'feedback']))

"""#### Plot Feedback Counts by Sentiment and Category
Visualize how many feedbacks are negative, positive, or neutral for each category (service, claim, policy, etc.).
"""

# Plot Feedback Counts by Sentiment and Category and save to reports/figures
plt.figure(figsize=(10, 6))
sns.countplot(x='category (text)', hue='sentiment (text)', data=df, order=df['category (text)'].value_counts().index)
plt.title('Feedback Counts by Sentiment and Category')
plt.xlabel('Category')
plt.ylabel('Number of Feedbacks')
plt.legend(title='Sentiment')
plt.show()

# Count negative feedbacks related to 'claim' category
negative_claim_count = df[(df['category (text)'] == 'claim') & (df['sentiment (text)'] == 'negative')].shape[0]
print(f"Number of negative feedbacks related to 'claim': {negative_claim_count}")

# Commented out IPython magic to ensure Python compatibility.
# %pip install --quiet transformers torch accelerate onnxscript

from sklearn.model_selection import train_test_split
import pandas as pd

# Load cleaned feedback data
df = pd.read_csv('/content/vehicle_feedback.csv')

# Ensure 'feedback_clean' column exists before splitting
import re
clean_pattern = r'[^\w\s.,!?@#$%&*()\-+=:;\'\"/\\]'
df['feedback_clean'] = df['feedback'].str.replace(clean_pattern, '', regex=True)

# Example: Use 'feedback_clean' as text, 'sentiment (text)' and 'category (text)' as labels
X = df['feedback_clean']
y_sentiment = df['sentiment (text)']
y_category = df['category (text)']

# Split for sentiment classification (repeat for category if needed)
X_train, X_test, y_train, y_test = train_test_split(X, y_sentiment, test_size=0.2, random_state=42, stratify=y_sentiment)
print(f'Train size: {len(X_train)}, Test size: {len(X_test)}')

from sklearn.model_selection import train_test_split
import pandas as pd

# Load cleaned feedback data
df = pd.read_csv('/content/vehicle_feedback.csv')

# Ensure 'feedback_clean' column exists before splitting
import re
# Corrected clean_pattern to match the one used in earlier cleaning steps
clean_pattern = r"[^a-zA-Z0-9\s.,!?'\"-]"
df['feedback_clean'] = df['feedback'].str.replace(clean_pattern, '', regex=True)

# Example: Use 'feedback_clean' as text, 'sentiment (text)' and 'category (text)' as labels
X = df['feedback_clean']
y_sentiment = df['sentiment (text)']
y_category = df['category (text)']

# Split for sentiment classification (repeat for category if needed)
X_train, X_test, y_train, y_test = train_test_split(X, y_sentiment, test_size=0.2, random_state=42, stratify=y_sentiment)
print(f'Train size: {len(X_train)}, Test size: {len(X_test)}')

# Phase 3.2: Tokenization for DistilBERT
from transformers import DistilBertTokenizerFast

tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

# Tokenize training and test data
train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=256)
test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=256)

print('Tokenization complete. Example:', train_encodings['input_ids'][0][:10])

import transformers
print(transformers.__version__)

# Use variables defined in previous cells: X, y_sentiment, y_category, X_train, X_test, y_train, y_test

# Set environment variable to control HuggingFace tokenizers parallelism
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# Phase 3.3: Fine-Tuning DistilBERT (Sentiment Classification)
import torch
from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments

# Prepare labels using y_sentiment from previous cells
label2id = {label: i for i, label in enumerate(sorted(y_sentiment.unique()))}
y_train_ids = [label2id[label] for label in y_train]
y_test_ids = [label2id[label] for label in y_test]

# Create torch dataset
class FeedbackDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels
    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item
    def __len__(self):
        return len(self.labels)

train_dataset = FeedbackDataset(train_encodings, y_train_ids)
test_dataset = FeedbackDataset(test_encodings, y_test_ids)

# Load model
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label2id))

# Training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=2,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    eval_strategy='epoch',
    save_strategy='epoch',
    logging_dir='./logs',
    logging_steps=50,
    load_best_model_at_end=True,
    metric_for_best_model='eval_loss',
    save_total_limit=2,
    fp16=False,  # Disable fp16 since it's not supported on CPU
    report_to='none',
    seed=42,
    disable_tqdm=False,
    do_train=True,
    do_eval=True,
    dataloader_num_workers=2,
    remove_unused_columns=True,
    run_name='distilbert-huk-feedback',
    optim='adamw_torch' # Changed from 'adamw_hf' to 'adamw_torch'
    # Add more as needed
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    # Add compute_metrics if needed
)

# Train
trainer.train()

# Phase 3.4: Model Evaluation (F1-score, Confusion Matrix)
from sklearn.metrics import f1_score, confusion_matrix, classification_report

# Use the trainer object defined in the previous cell
if 'trainer' not in globals():
    raise NameError("Error: 'trainer' is not defined. Please run the training cell (Phase 3.3) before evaluating the model.")

preds = trainer.predict(test_dataset)
y_pred = preds.predictions.argmax(axis=1)

# F1-score
f1 = f1_score(y_test_ids, y_pred, average='weighted')
print('Weighted F1-score:', f1)

# Confusion matrix
cm = confusion_matrix(y_test_ids, y_pred)
print('Confusion Matrix:\n', cm)

# Classification report
print(classification_report(y_test_ids, y_pred, target_names=label2id.keys()))

# Phase 3.5: Export Trained Model to ONNX
from transformers import DistilBertForSequenceClassification, DistilBertTokenizer
import torch
import os

# Reload best model if needed (this part was already fixed, keeping for context)
model.save_pretrained('./models/distilbert_huk_sentiment', safe_serialization=False)

# Load tokenizer for proper dummy input generation
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# Set model to evaluation mode
model.eval()

# Create proper dummy inputs that match the model's expected signature
# DistilBERT models typically expect 'input_ids' and 'attention_mask'
dummy_text = "This is a dummy input sentence for ONNX export."
dummy_inputs = tokenizer(dummy_text, return_tensors="pt", truncation=True, padding='max_length', max_length=256)
input_ids = dummy_inputs['input_ids'].to(model.device)
attention_mask = dummy_inputs['attention_mask'].to(model.device)

# The model's forward pass expects (input_ids, attention_mask)
model_args = (input_ids, attention_mask)

# Export to ONNX
output_path = '../models/huk_distilbert.onnx'

# Create the directory if it does not exist
os.makedirs(os.path.dirname(output_path), exist_ok=True)

torch.onnx.export(
    model,
    model_args, # Pass the tuple of inputs (input_ids, attention_mask)
    output_path,
    input_names=['input_ids', 'attention_mask'], # Specify all input names
    output_names=['logits'], # Model's output is typically 'logits' for classification
    dynamic_axes={
        'input_ids': {0: 'batch_size', 1: 'sequence_length'}, # Make sequence_length dynamic
        'attention_mask': {0: 'batch_size', 1: 'sequence_length'}, # Make attention_mask's sequence_length dynamic
        'logits': {0: 'batch_size'} # Output batch_size can also be dynamic
    },
    opset_version=18, # Use opset_version 18 as suggested by the warning
    do_constant_folding=True,
    dynamo=False # Explicitly disable dynamo to try the older tracing path
)
print(f'Model exported to ONNX at {output_path}')

!zip -r /content/models.zip ../models
!zip -r /content/results.zip ./results

"""### 1. Install ONNX Runtime

First, make sure you have `onnxruntime` installed.
"""

pip install onnxruntime

"""### 2. Load the ONNX model and tokenizer

Load the `onnxruntime` session and the `DistilBertTokenizerFast`.
"""

import onnxruntime as ort
from transformers import DistilBertTokenizerFast
import numpy as np

# Load the ONNX model
onnx_model_path = '../models/huk_distilbert.onnx'
session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])

# Load the tokenizer (same as used during training)
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

# Recreate the label mapping
# Assuming `label2id` was created as {'negative': 0, 'neutral': 1, 'positive': 2}
# We need the inverse mapping for output
id2label = {0: 'negative', 1: 'neutral', 2: 'positive'}

print("ONNX model and tokenizer loaded successfully!")

"""### 3. Prepare an example input and make a prediction

Let's take a sample feedback text, tokenize it, and pass it to the ONNX model for prediction. The model's output will be logits, which we'll convert to a probability distribution and then to the predicted label.
"""

def predict_sentiment(text):
    # Tokenize the input text
    inputs = tokenizer(text, return_tensors="np", truncation=True, padding='max_length', max_length=256)

    # Prepare inputs for ONNX session
    onnx_inputs = {
        'input_ids': inputs['input_ids'].astype(np.int64),
        'attention_mask': inputs['attention_mask'].astype(np.int64)
    }

    # Run inference
    outputs = session.run(None, onnx_inputs)
    logits = outputs[0] # The output is typically logits

    # Apply softmax to get probabilities (optional, but good for understanding confidence)
    exp_logits = np.exp(logits - np.max(logits, axis=-1, keepdims=True)) # for numerical stability
    probabilities = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)

    # Get the predicted label ID
    predicted_id = np.argmax(logits, axis=-1)[0]

    # Map ID back to label
    predicted_label = id2label[predicted_id]

    return predicted_label, probabilities[0]

# Example usage:
sample_feedback = "I am extremely satisfied with the excellent service provided. The staff was very helpful and resolved my issue quickly."
predicted_sentiment, probs = predict_sentiment(sample_feedback)
print(f"Sample Feedback: {sample_feedback}")
print(f"Predicted Sentiment: {predicted_sentiment}")
print(f"Probabilities: {probs}")

sample_feedback_negative = "I'm very disappointed with the long waiting times and unresponsive customer support."
predicted_sentiment_negative, probs_negative = predict_sentiment(sample_feedback_negative)
print(f"\nSample Feedback: {sample_feedback_negative}")
print(f"Predicted Sentiment: {predicted_sentiment_negative}")
print(f"Probabilities: {probs_negative}")

sample_feedback_neutral = "The policy terms are standard and the process was neither good nor bad."
predicted_sentiment_neutral, probs_neutral = predict_sentiment(sample_feedback_neutral)
print(f"\nSample Feedback: {sample_feedback_neutral}")
print(f"Predicted Sentiment: {predicted_sentiment_neutral}")
print(f"Probabilities: {probs_neutral}")

"""After executing the cell above, you can download the zipped files (`models.zip` and `results.zip`) from the Colab file browser (usually located on the left-hand side panel).

If the files don't appear immediately, you might need to refresh the file browser.

# Task
The models and results folders have been successfully re-zipped and are available for download.

The task of training the sentiment analysis model, converting it to ONNX, and testing the ONNX model is now complete.

## Re-zip models and results folders

### Subtask:
Execute the cell that zips the `../models` and `./results` directories into `/content/models.zip` and `/content/results.zip` respectively.

## Summary:

### Data Analysis Key Findings
*   The `models` and `results` directories were successfully re-zipped into `/content/models.zip` and `/content/results.zip`, respectively, and are now available for download.
*   The entire pipeline, including the training of the sentiment analysis model, its conversion to ONNX format, and the testing of the ONNX model, has been completed.

### Insights or Next Steps
*   The successful completion of the model development and testing phases indicates the sentiment analysis model is ready for potential deployment or integration into a larger system.
*   The re-zipped `models` and `results` archives provide a convenient and consolidated package for transferring or archiving the project's outputs.
"""

!zip -r /content/other_folders.zip ./.config ./.ipynb_checkpoints